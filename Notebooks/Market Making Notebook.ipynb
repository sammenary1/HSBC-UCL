{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b918608",
   "metadata": {},
   "source": [
    "### Import external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffdffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T20:01:39.869599Z",
     "start_time": "2023-06-12T20:01:30.190346Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna.visualization as vis\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0a2c9",
   "metadata": {},
   "source": [
    "### Add mbt-gym to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387934ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T20:01:39.877016Z",
     "start_time": "2023-06-12T20:01:39.872840Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb89dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T20:01:39.917980Z",
     "start_time": "2023-06-12T20:01:39.887676Z"
    }
   },
   "outputs": [],
   "source": [
    "from mbt_gym.agents.BaselineAgents import CarteaJaimungalMmAgent\n",
    "from mbt_gym.gym.helpers.generate_trajectory import generate_trajectory\n",
    "from mbt_gym.gym.StableBaselinesTradingEnvironment import StableBaselinesTradingEnvironment\n",
    "from mbt_gym.gym.TradingEnvironment import TradingEnvironment\n",
    "from mbt_gym.gym.wrappers import *\n",
    "from mbt_gym.rewards.RewardFunctions import PnL, CjMmCriterion\n",
    "from mbt_gym.stochastic_processes.midprice_models import *\n",
    "from mbt_gym.stochastic_processes.arrival_models import PoissonArrivalModel\n",
    "from mbt_gym.stochastic_processes.fill_probability_models import ExponentialFillFunction\n",
    "from mbt_gym.gym.ModelDynamics import LimitOrderModelDynamics\n",
    "from mbt_gym.gym.helpers.plotting import generate_trajectory, generate_results_table_and_hist, plot_trajectory\n",
    "from mbt_gym.agents.SbAgent import SbAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d65b0",
   "metadata": {},
   "source": [
    "### Create market making environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab1846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T20:01:39.923661Z",
     "start_time": "2023-06-12T20:01:39.920255Z"
    }
   },
   "outputs": [],
   "source": [
    "sigma= 0.1 # constant \"volatility\" of mid-price process\n",
    "arrival_rate = 10.0 # lambda \n",
    "fill_exponent = 1 # kappa\n",
    "alpha = 0.001 # terminal inventory penalty (fees of market orders and walking the book)\n",
    "phi = 0.5 # running inventory penalty parameter\n",
    "\n",
    "terminal_time = 1.0 # time [0,1]\n",
    "max_inventory = 3\n",
    "initial_inventory = (-3,4) # initial inventory will be random integer from {-3,-2,...,2,3}\n",
    "initial_price = 100\n",
    "\n",
    "n_steps = int(10 * terminal_time * arrival_rate)\n",
    "step_size = 1/n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11432746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T20:01:39.965190Z",
     "start_time": "2023-06-12T20:01:39.926568Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cj_env(num_trajectories:int = 1):\n",
    "    timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "    midprice_model = BrownianMotionMidpriceModel(step_size=1/n_steps,\n",
    "                                                 num_trajectories=num_trajectories)\n",
    "    arrival_model = PoissonArrivalModel(intensity=np.array([arrival_rate, arrival_rate]), \n",
    "                                        step_size=1/n_steps, \n",
    "                                        num_trajectories=num_trajectories)\n",
    "    fill_probability_model = ExponentialFillFunction(fill_exponent=fill_exponent, \n",
    "                                                     step_size=1/n_steps,\n",
    "                                                     num_trajectories=num_trajectories)\n",
    "    LOtrader = LimitOrderModelDynamics(midprice_model = midprice_model, arrival_model = arrival_model, \n",
    "                                fill_probability_model = fill_probability_model,\n",
    "                                num_trajectories = num_trajectories)\n",
    "    reward_function = CjMmCriterion(per_step_inventory_aversion = phi, terminal_inventory_aversion = alpha)\n",
    "    env_params = dict(terminal_time=terminal_time, \n",
    "                      n_steps=n_steps,\n",
    "                      initial_inventory = initial_inventory,\n",
    "                      model_dynamics = LOtrader,\n",
    "                      max_inventory=n_steps,\n",
    "                      normalise_action_space = False,\n",
    "                      normalise_observation_space = False,\n",
    "                      reward_function = reward_function,\n",
    "                      num_trajectories=num_trajectories)\n",
    "    return TradingEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29022e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T20:01:39.965825Z",
     "start_time": "2023-06-12T20:01:39.945928Z"
    }
   },
   "outputs": [],
   "source": [
    "num_trajectories = 1000\n",
    "env = ReduceStateSizeWrapper(get_cj_env(num_trajectories))\n",
    "sb_env = StableBaselinesTradingEnvironment(trading_env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f837dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T20:01:39.966047Z",
     "start_time": "2023-06-12T20:01:39.958418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monitor sb_env\n",
    "sb_env = VecMonitor(sb_env)\n",
    "# Add directory for tensorboard logging and best model\n",
    "tensorboard_logdir = \"./tensorboard/RL-learning-CJ/\"\n",
    "best_model_path = \"./SB_models/RL-best-CJ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106df91",
   "metadata": {},
   "source": [
    "### Train PPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893adefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logdir = \"./tensorboard/\"\n",
    "best_model_path = \"./SB_models/\"\n",
    "n_eval_episodes = 50\n",
    "eval_freq = 500_000\n",
    "total_timesteps = 20_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_tensorboard_logdir = tensorboard_logdir + \"PPO-learning-CJ/\"\n",
    "PPO_best_model_path = best_model_path + \"PPO-best-CJ\"\n",
    "PPO_callback_params = dict(eval_env=sb_env, n_eval_episodes=n_eval_episodes, best_model_save_path=PPO_best_model_path, eval_freq = eval_freq,\n",
    "deterministic=True)\n",
    "\n",
    "PPO_callback = EvalCallback(**PPO_callback_params)\n",
    "ppo_model = PPO(\"MlpPolicy\", sb_env, verbose=0, tensorboard_log=PPO_tensorboard_logdir, n_steps= int(n_steps), batch_size= int(n_steps * num_trajectories / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model.learn(total_timesteps=total_timesteps, callback = PPO_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d93ff",
   "metadata": {},
   "source": [
    "### Train A2C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a064e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C\n",
    "A2C_tensorboard_logdir = tensorboard_logdir + \"A2C-learning-CJ/\"\n",
    "A2C_best_model_path = best_model_path + \"A2C-best-CJ\"\n",
    "A2C_callback_params = dict(eval_env=sb_env, n_eval_episodes=n_eval_episodes, \n",
    "                           best_model_save_path=A2C_best_model_path, \n",
    "                           deterministic=True)\n",
    "\n",
    "A2C_callback = EvalCallback(**A2C_callback_params)\n",
    "a2c_model = A2C(\"MlpPolicy\", sb_env, verbose=0, n_steps = 3, tensorboard_log=A2C_tensorboard_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_model.learn(total_timesteps=total_timesteps, callback=A2C_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d80cb",
   "metadata": {},
   "source": [
    "### Train SAC Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC\n",
    "SAC_tensorboard_logdir = tensorboard_logdir + \"SAC-learning-CJ/\"\n",
    "SAC_best_model_path = best_model_path + \"SAC-best-CJ\"\n",
    "SAC_callback_params = dict(eval_env=sb_env, n_eval_episodes=n_eval_episodes, \n",
    "                           best_model_save_path=SAC_best_model_path, \n",
    "                           deterministic=True)\n",
    "\n",
    "SAC_callback = EvalCallback(**SAC_callback_params)\n",
    "sac_model = SAC(\"MlpPolicy\", sb_env, verbose=0, batch_size = 256, tensorboard_log=SAC_tensorboard_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_model.learn(total_timesteps=total_timesteps, callback=SAC_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48dc22e",
   "metadata": {},
   "source": [
    "### Creating RL Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = SbAgent(ppo_model)\n",
    "sac_agent = SbAgent(sac_model)\n",
    "a2c_agent = SbAgent(a2c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = np.arange(-3, 4, 1)\n",
    "\n",
    "# Collect PPO actions\n",
    "ppo_bid_actions, ppo_ask_actions = [], []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = ppo_agent.get_action(np.array([[inventory, 0.5]])).reshape(-1)\n",
    "    ppo_bid_actions.append(bid_action)\n",
    "    ppo_ask_actions.append(ask_action)\n",
    "\n",
    "# Collect SAC actions\n",
    "sac_bid_actions, sac_ask_actions = [], []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = sac_agent.get_action(np.array([[inventory, 0.5]])).reshape(-1)\n",
    "    sac_bid_actions.append(bid_action)\n",
    "    sac_ask_actions.append(ask_action)\n",
    "\n",
    "# Collect A2C actions\n",
    "a2c_bid_actions, a2c_ask_actions = [], []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = a2c_agent.get_action(np.array([[inventory, 0.5]])).reshape(-1)\n",
    "    a2c_bid_actions.append(bid_action)\n",
    "    a2c_ask_actions.append(ask_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543aadb6",
   "metadata": {},
   "source": [
    "### Creating Optimal Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_agent = CarteaJaimungalMmAgent(env=get_cj_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Cartea Jaimungal action\n",
    "cj_bid_actions = []\n",
    "cj_ask_actions = []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = cj_agent.get_action(np.array([[0,inventory,0.5]])).reshape(-1)\n",
    "    cj_bid_actions.append(bid_action)\n",
    "    cj_ask_actions.append(ask_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd8a13",
   "metadata": {},
   "source": [
    "### Plotting Depth vs Inventory Plots for Agent Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42473057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.plot(inventories, ppo_bid_actions, label=\"PPO bid\", color=\"g\",linewidth=0.8)\n",
    "plt.plot(inventories, ppo_ask_actions, label=\"PPO ask\", color=\"g\",linewidth=0.8)\n",
    "\n",
    "plt.plot(inventories, sac_bid_actions, label=\"SAC bid\", color=\"y\", linewidth=0.8)\n",
    "plt.plot(inventories, sac_ask_actions, label=\"SAC ask\", color=\"y\",linewidth=0.8)\n",
    "\n",
    "plt.plot(inventories, a2c_bid_actions, label=\"A2C bid\", color=\"b\",linewidth=0.8)\n",
    "plt.plot(inventories, a2c_ask_actions, label=\"A2C ask\", color=\"b\",linewidth=0.8)\n",
    "\n",
    "plt.plot(inventories, cj_bid_actions, label=\"CJ bid\", color=\"k\", linestyle=\"--\", linewidth=2.5)\n",
    "plt.plot(inventories, cj_ask_actions, label=\"CJ ask\", color=\"r\", linestyle=\"--\", linewidth=2.5)\n",
    "# Adding title and axis labels\n",
    "plt.title(\"Depth vs Inventory\")\n",
    "plt.xlabel(\"Inventory\")\n",
    "plt.ylabel(\"Depth\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1dcb5",
   "metadata": {},
   "source": [
    "### Plotting MSE for Agent Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_ppo_bid = mean_squared_error(cj_bid_actions, ppo_bid_actions)\n",
    "mse_ppo_ask = mean_squared_error(cj_ask_actions, ppo_ask_actions)\n",
    "\n",
    "# Do the same for SAC and A2C\n",
    "mse_a2c_bid = mean_squared_error(cj_bid_actions, a2c_bid_actions)\n",
    "mse_a2c_ask = mean_squared_error(cj_ask_actions, a2c_ask_actions)\n",
    "\n",
    "mse_sac_bid = mean_squared_error(cj_bid_actions, sac_bid_actions)\n",
    "mse_sac_ask = mean_squared_error(cj_ask_actions, sac_ask_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24515e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PPO', 'A2C', 'SAC']\n",
    "bid_mse_values = [mse_ppo_bid, mse_a2c_bid, mse_sac_bid]\n",
    "ask_mse_values = [mse_ppo_ask, mse_a2c_ask, mse_sac_ask]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, bid_mse_values, width, label='Bid MSE')\n",
    "rects2 = ax.bar(x + width/2, ask_mse_values, width, label='Ask MSE')\n",
    "\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE values by policy and action type')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50ac12",
   "metadata": {},
   "source": [
    "### Optimising PPO Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "def objective_ppo_mse(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.9, 0.9999)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    vf_coef = trial.suggest_float(\"vf_coef\", 0.1, 1.0)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 1e-8, 1e-1, log=True)\n",
    "    clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.4)\n",
    "\n",
    "    # Create the PPO model with the specified hyperparameters\n",
    "    ppo_model_mse = PPO(\"MlpPolicy\", sb_env, gamma=gamma, learning_rate=learning_rate, \n",
    "                vf_coef=vf_coef, ent_coef=ent_coef, clip_range=clip_range, verbose=0, n_steps= int(n_steps), batch_size= int(n_steps * num_trajectories / 10))\n",
    "    \n",
    "    # Train the model\n",
    "    ppo_model_mse.learn(total_timesteps=20_000_000)\n",
    "    \n",
    "    # Create agent and evaluate\n",
    "    agent = SbAgent(ppo_model_mse)\n",
    "    bid_actions, ask_actions = [], []\n",
    "    for inventory in inventories:\n",
    "        bid_action, ask_action = agent.get_action(np.array([[inventory, 0.5]])).reshape(-1)\n",
    "        bid_actions.append(bid_action)\n",
    "        ask_actions.append(ask_action)\n",
    "\n",
    "    mse = np.mean((np.array(bid_actions) - np.array(cj_bid_actions))**2 +\n",
    "                  (np.array(ask_actions) - np.array(cj_ask_actions))**2)\n",
    "\n",
    "    return -mse\n",
    "\n",
    "# Initiate the study object\n",
    "study_ppo_mse = optuna.create_study(direction=\"maximize\")  # We want to maximize the objective\n",
    "\n",
    "# Run optimization\n",
    "study_ppo_mse.optimize(objective_ppo_mse, n_trials=25)  # Adjust n_trials based on your computational budget\n",
    "\n",
    "# Print the result\n",
    "best_params = study_ppo_mse.best_params\n",
    "best_value = study_ppo_mse.best_value\n",
    "print(f\"Best value: {best_value}\\nWith parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35b2ed",
   "metadata": {},
   "source": [
    "### Visualising Optimisation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "%matplotlib inline\n",
    "# 1. Optimization History\n",
    "vis.plot_optimization_history(study_ppo_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parallel Coordinate Plot\n",
    "vis.plot_parallel_coordinate(study_ppo_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385778e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Slice Plot\n",
    "vis.plot_slice(study_ppo_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Contour Plot\n",
    "# For this, you might want to pick two hyperparameters to visualize, e.g., \"gamma\" and \"learning_rate\"\n",
    "vis.plot_contour(study_ppo_mse, params=[\"gamma\", \"ent_coef\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06309fc6",
   "metadata": {},
   "source": [
    "### Training Model Using Best Hyperparameters from Optimisation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_ppo_mse = PPO(\"MlpPolicy\", sb_env, **best_params, verbose=0, n_steps= int(n_steps), batch_size= int(n_steps * num_trajectories / 10), tensorboard_log=PPO_tensorboard_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_ppo_mse.learn(total_timesteps=20_000_000, callback=PPO_callback)  # You can adjust the number of timesteps based on your requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fae304",
   "metadata": {},
   "source": [
    "### Creating New Agent Using Optimised Model and Plotting Same Plots as Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mse_agent = SbAgent(best_model_ppo_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect PPO actions\n",
    "ppo_mse_bid_actions, ppo_mse_ask_actions = [], []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = ppo_mse_agent.get_action(np.array([[inventory, 0.5]])).reshape(-1)\n",
    "    ppo_mse_bid_actions.append(bid_action)\n",
    "    ppo_mse_ask_actions.append(ask_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74703d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inventories, ppo_bid_actions, label=\"PPO Bid\", color=\"y\",linewidth=0.8)\n",
    "plt.plot(inventories, ppo_ask_actions, label=\"PPO Ask\", color=\"y\",linewidth=0.8)\n",
    "\n",
    "plt.plot(inventories, ppo_mse_bid_actions, label=\"PPO MSE Opt. Bid\", color=\"b\",linewidth=0.8)\n",
    "plt.plot(inventories, ppo_mse_ask_actions, label=\"PPO MSE Opt. Ask\", color=\"b\",linewidth=0.8)\n",
    "\n",
    "plt.plot(inventories, cj_bid_actions, label=\"CJ Bid\", color=\"k\", linestyle=\"--\", linewidth=2.5)\n",
    "plt.plot(inventories, cj_ask_actions, label=\"CJ Ask\", color=\"r\", linestyle=\"--\", linewidth=2.5)\n",
    "# Adding title and axis labels\n",
    "plt.title(\"Depth vs Inventory\")\n",
    "plt.xlabel(\"Inventory\")\n",
    "plt.ylabel(\"Depth\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ppo_bid_mse = mean_squared_error(cj_bid_actions, ppo_mse_bid_actions)\n",
    "mse_ppo_ask_mse = mean_squared_error(cj_ask_actions, ppo_mse_ask_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f950720",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PPO No Opt.','PPO MSE Opt.']\n",
    "bid_mse_values = [mse_ppo_bid, mse_ppo_bid_mse]\n",
    "ask_mse_values = [mse_ppo_ask, mse_ppo_ask_mse]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, bid_mse_values, width, label='Bid MSE')\n",
    "rects2 = ax.bar(x + width/2, ask_mse_values, width, label='Ask MSE')\n",
    "\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE values by optimisation method')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca43a78",
   "metadata": {},
   "source": [
    "### Performing 2nd Optimisation Study for Architecture Hyperparameters Using Best Hyperparameters from the Last Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779aca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best hyperparameters from the previous study\n",
    "best_params_mse = study_ppo_mse.best_params\n",
    "\n",
    "PPO_activation_functions_to_test = ['Tanh', 'ReLU', 'Sigmoid', 'LeakyReLU']  # Note the capitalization for torch.nn functions\n",
    "PPO_num_layers_to_test = [1, 2, 3]\n",
    "PPO_hidden_units_to_test = [32, 64, 128, 256]\n",
    "\n",
    "def objective_ppo_architecture(trial):\n",
    "    # Use the best hyperparameters from the previous optimization\n",
    "    gamma = best_params_mse[\"gamma\"]\n",
    "    learning_rate = best_params_mse[\"learning_rate\"]\n",
    "    vf_coef = best_params_mse[\"vf_coef\"]\n",
    "    ent_coef = best_params_mse[\"ent_coef\"]\n",
    "    clip_range = best_params_mse[\"clip_range\"]\n",
    "\n",
    "    # Now optimize the new parameters\n",
    "    activation_function = trial.suggest_categorical(\"activation_function\", PPO_activation_functions_to_test)\n",
    "    n_layers = trial.suggest_categorical(\"n_layers\", PPO_num_layers_to_test)\n",
    "    hidden_units = trial.suggest_categorical(\"hidden_units\", PPO_hidden_units_to_test)\n",
    "\n",
    "    layers = [hidden_units] * n_layers  # Repeating the chosen number of units for n_layers times\n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        net_arch=[dict(pi=layers, vf=layers)],\n",
    "        activation_fn=getattr(torch.nn, activation_function)\n",
    "    )\n",
    "\n",
    "    # Create the PPO model with the specified parameters\n",
    "    ppo_model_arch = PPO(\"MlpPolicy\", sb_env, gamma=gamma, learning_rate=learning_rate, \n",
    "                vf_coef=vf_coef, ent_coef=ent_coef, clip_range=clip_range, verbose=0, \n",
    "                n_steps=int(n_steps), batch_size=int(n_steps * num_trajectories / 10),\n",
    "                policy_kwargs=policy_kwargs)\n",
    "\n",
    "    # Train the model\n",
    "    ppo_model_arch.learn(total_timesteps=20_000_000)\n",
    "    \n",
    "    # Create agent and evaluate\n",
    "    agent = SbAgent(ppo_model_arch)\n",
    "    bid_actions, ask_actions = [], []\n",
    "    for inventory in inventories:\n",
    "        bid_action, ask_action = agent.get_action(np.array([[inventory, 0.5]])).reshape(-1)\n",
    "        bid_actions.append(bid_action)\n",
    "        ask_actions.append(ask_action)\n",
    "\n",
    "    mse = np.mean((np.array(bid_actions) - np.array(cj_bid_actions))**2 +\n",
    "                  (np.array(ask_actions) - np.array(cj_ask_actions))**2)\n",
    "\n",
    "    return -mse  # Assuming you want to maximize the negative mean squared error again\n",
    "\n",
    "# New Optuna study\n",
    "study_ppo_architecture = optuna.create_study(direction=\"maximize\")\n",
    "study_ppo_architecture.optimize(objective_ppo_architecture, n_trials=25)\n",
    "\n",
    "# Print the result\n",
    "best_params_architecture = study_ppo_architecture.best_params\n",
    "best_value_architecture = study_ppo_architecture.best_value\n",
    "print(f\"Best value: {best_value_architecture}\\nWith parameters: {best_params_architecture}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best parameters\n",
    "best_arch_params = study_ppo_architecture.best_params\n",
    "\n",
    "# Define architecture based on the best parameters\n",
    "activation_function = getattr(torch.nn, best_arch_params['activation_function'])\n",
    "hidden_units = best_arch_params['hidden_units']\n",
    "n_layers = best_arch_params['n_layers']\n",
    "\n",
    "# Construct the architecture using the best number of layers and units\n",
    "layers = [hidden_units] * n_layers\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=layers, vf=layers)],\n",
    "    activation_fn=activation_function\n",
    ")\n",
    "\n",
    "# The **all_best_params will unpack and use both sets of parameters.\n",
    "best_model_ppo_architecture = PPO(\"MlpPolicy\", sb_env, **best_params, policy_kwargs=policy_kwargs, verbose=0, n_steps=int(n_steps), batch_size=int(n_steps * num_trajectories / 10), tensorboard_log=PPO_tensorboard_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_ppo_architecture.learn(total_timesteps=20_000_000, callback=PPO_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f5745",
   "metadata": {},
   "source": [
    "### Plotting Same Graphs as Before Using the Final Optimised Agent, Agent with no Optimisation, and Optimal Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb988cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_arc_agent = SbAgent(best_model_ppo_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect PPO actions\n",
    "ppo_arc_bid_actions, ppo_arc_ask_actions = [], []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = ppo_arc_agent.get_action(np.array([[inventory, 0.5]])).reshape(-1)\n",
    "    ppo_arc_bid_actions.append(bid_action)\n",
    "    ppo_arc_ask_actions.append(ask_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9129822",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inventories, ppo_bid_actions, label=\"PPO Bid\", color=\"y\",linewidth=0.8)\n",
    "plt.plot(inventories, ppo_ask_actions, label=\"PPO Ask\", color=\"y\",linewidth=0.8)\n",
    "\n",
    "plt.plot(inventories, ppo_arc_bid_actions, label=\"Optimised PPO Bid\", color=\"b\",linewidth=0.8)\n",
    "plt.plot(inventories, ppo_arc_ask_actions, label=\"Optimised PPO Ask\", color=\"b\",linewidth=0.8)\n",
    "\n",
    "plt.plot(inventories, cj_bid_actions, label=\"CJ Bid\", color=\"k\", linestyle=\"--\", linewidth=2.5)\n",
    "plt.plot(inventories, cj_ask_actions, label=\"CJ Ask\", color=\"r\", linestyle=\"--\", linewidth=2.5)\n",
    "# Adding title and axis labels\n",
    "plt.title(\"Depth vs Inventory\")\n",
    "plt.xlabel(\"Inventory\")\n",
    "plt.ylabel(\"Depth\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_ppo_bid_mse = mean_squared_error(cj_bid_actions, ppo_arc_bid_actions)\n",
    "arc_ppo_ask_mse = mean_squared_error(cj_ask_actions, ppo_arc_ask_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c256554",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PPO No Opt.','Optimised PPO']\n",
    "bid_mse_values = [mse_ppo_bid, arc_ppo_bid_mse]\n",
    "ask_mse_values = [mse_ppo_ask, arc_ppo_ask_mse]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, bid_mse_values, width, label='Bid MSE')\n",
    "rects2 = ax.bar(x + width/2, ask_mse_values, width, label='Ask MSE')\n",
    "\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE Before and After Optimisation')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
